<h3 align="left">Hi ğŸ‘‹ I'm Chika Maduabuchi, an ML researcher working on multimodal generative AI and video understanding</h3>

<h3 align="left">ğŸš€ Ongoing/Completed Projects</h3>

1) **CAT-LVDM**: *Corruption-Aware Training of Latent Video Diffusion Models* for robust text-to-video generation  
ğŸ“ NeurIPS 2025 (under review) Â· [ğŸ“„ arXiv](https://arxiv.org/abs/2505.21545) Â· [ğŸ§  Code](https://github.com/chikap421/catlvdm) Â· [ğŸ¤— Checkpoints](https://huggingface.co/Chikap421/catlvdm-checkpoints/tree/main)

<div align="center">
  <img src="assets/catlvdm.png" width="500"/>
</div>

---

2) **VideoSAM**: A large vision foundation model for **high-speed video segmentation**  
ğŸ“ IEEE SSD 2025 Â· [ğŸ“„ arXiv](https://arxiv.org/abs/2410.21304) Â· [ğŸ§  Code](https://github.com/chikap421/videosam)

<div align="center">
  <img src="assets/videosam_teaser_plot.png" width="500"/>
</div>

---

3) **AfriInstruct**: Instruction tuning of African languages for diverse NLP tasks  
ğŸ“ EMNLP 2024 Â· [ğŸ“„ Paper](https://aclanthology.org/2024.findings-emnlp.793/) Â· [ğŸ§  Code](https://github.com/chikap421/AfriInstruct)

<div align="center">
  <img src="assets/afriinstruct_teaser.png" width="500"/>
</div>

---

4) **Auto-regressive Video Generation**: *Corruption-aware training of autoregressive models* for robust generation  
ğŸ“ ICLR 2026 (to be submitted)

---

5) **Robust Multimodal Video-Language Models**: Side-channel fusion of corrupted inputs  
ğŸ“ CVPR 2026 (to be submitted)

---

<h3 align="left">ğŸ”— Links</h3>

- ğŸ‘¨â€ğŸ“ Google Scholar: [scholar.google.com/citations?user=YrLydoQAAAAJ](https://scholar.google.com/citations?user=YrLydoQAAAAJ&hl=en)  
- ğŸ“§ Email: [chikap421@gmail.com](mailto:chikap421@gmail.com)  
- ğŸ”— LinkedIn: [linkedin.com/in/mchika](https://www.linkedin.com/in/mchika/)  
- ğŸ§  Hugging Face: [huggingface.co/Chikap421](https://huggingface.co/Chikap421)  
