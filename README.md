<h3 align="left">Hi 👋 I'm Chika Maduabuchi, an ML researcher working on multimodal generative AI and video understanding</h3>

<h3 align="left">🚀 Ongoing/Completed Projects</h3>

1) **CAT-LVDM**: *Corruption-Aware Training of Latent Video Diffusion Models* for robust text-to-video generation  
📍 NeurIPS 2025 (under review) · [📄 arXiv](https://arxiv.org/abs/2505.21545) · [🧠 Code](https://github.com/chikap421/catlvdm) · [🤗 Checkpoints](https://huggingface.co/Chikap421/catlvdm-checkpoints/tree/main)

<div align="center">
  <img src="assets/catlvdm.png" width="500"/>
</div>

---

2) **VideoSAM**: A large vision foundation model for **high-speed video segmentation**  
📍 IEEE SSD 2025 · [📄 arXiv](https://arxiv.org/abs/2410.21304) · [🧠 Code](https://github.com/chikap421/videosam)

<div align="center">
  <img src="assets/videosam_teaser_plot.png" width="500"/>
</div>

---

3) **AfriInstruct**: Instruction tuning of African languages for diverse NLP tasks  
📍 EMNLP 2024 · [📄 Paper](https://aclanthology.org/2024.findings-emnlp.793/) · [🧠 Code](https://github.com/chikap421/AfriInstruct)

<div align="center">
  <img src="assets/afriinstruct_teaser.png" width="500"/>
</div>

---

4) **Auto-regressive Video Generation**: *Corruption-aware training of autoregressive models* for robust generation  
📍 ICLR 2026 (to be submitted)

---

5) **Robust Multimodal Video-Language Models**: Side-channel fusion of corrupted inputs  
📍 CVPR 2026 (to be submitted)

---

<h3 align="left">🔗 Links</h3>

- 👨‍🎓 Google Scholar: [scholar.google.com/citations?user=YrLydoQAAAAJ](https://scholar.google.com/citations?user=YrLydoQAAAAJ&hl=en)  
- 📧 Email: [chikap421@gmail.com](mailto:chikap421@gmail.com)  
- 🔗 LinkedIn: [linkedin.com/in/mchika](https://www.linkedin.com/in/mchika/)  
- 🧠 Hugging Face: [huggingface.co/Chikap421](https://huggingface.co/Chikap421)  
